{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4836fe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import PIL\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.models import Sequential\n",
    "#Import from keras_preprocessing not from keras.preprocessing\n",
    "from keras_preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import regularizers, optimizers\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, BatchNormalization, Conv2D, Dense, Dropout, Flatten, MaxPooling2D,GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "#### Necessary Imports for Neural Net\n",
    "## Some redundant ones in here too\n",
    "\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, ZeroPadding2D, Flatten, BatchNormalization, AveragePooling2D, Dense, Activation, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# # Helper functions for visualization:\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ae8370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    From scikit-learn: plots a confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    #fixes \"squishing of plot\"\n",
    "    plt.ylim([1.5, -.5])\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "def plot_model_history(history, n_epochs):\n",
    "    '''Plot the training and validation history for a TensorFlow network'''\n",
    "\n",
    "    # Extract loss and accuracy\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(10,5))\n",
    "    ax[0].plot(np.arange(n_epochs), loss, label='Training Loss')\n",
    "    ax[0].plot(np.arange(n_epochs), val_loss, label='Validation Loss')\n",
    "    ax[0].set_title('Loss Curves')\n",
    "    ax[0].legend()\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "\n",
    "    ax[1].plot(np.arange(n_epochs), acc, label='Training Accuracy')\n",
    "    ax[1].plot(np.arange(n_epochs), val_acc, label='Validation Accuracy')\n",
    "    ax[1].set_title('Accuracy Curves')\n",
    "    ax[1].legend()\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].set_ylabel('Accuracy')\n",
    "    \n",
    "    \n",
    "# # Check balance of labels/data in dataframe\n",
    "\n",
    "def checkBalance(df):\n",
    "    all_labels = df['labels']\n",
    "    all_labels = all_labels.tolist()\n",
    "    balance = df['labels'].value_counts()\n",
    "    print(balance)\n",
    "    for i in range(len(balance)):\n",
    "        print(f'{balance[i]*100/df.size:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5db5b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(232, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0 = pd.read_fwf('PCC_cat.txt', header=None)\n",
    "# df0[21] # 21 is the label entry index\n",
    "\n",
    "\n",
    "# # Here we have 7 unique labels:\n",
    "labels = np.unique(df0[21])\n",
    "\n",
    "# access ra and dec from their columns in the dataframe\n",
    "ra = df0[2]\n",
    "dec = df0[3]\n",
    "\n",
    "# want only bright objects above r_mag < 19.4  (the magnitude decreases as brightness increases)\n",
    "bright = np.where(df0[4] <= 19.4)\n",
    "brightDF = df0.iloc[bright].copy()\n",
    "\n",
    "labels = np.unique(brightDF[21])\n",
    "\n",
    "# access ra and dec from their columns in the dataframe\n",
    "ra = brightDF[2]\n",
    "dec = brightDF[3]\n",
    "\n",
    "filenames = []\n",
    "for r, d in zip(ra, dec):\n",
    "    fn = f'sdss_ra={r}_dec={d}.png'\n",
    "    filenames.append(fn)\n",
    "\n",
    "brightDF_reduced = pd.DataFrame({'files' : filenames,\n",
    "                                 'labels': brightDF[21]})\n",
    "\n",
    "\n",
    "# checkBalance(brightDF_reduced)\n",
    "df1 = brightDF_reduced\n",
    "unique_labels = np.unique(df1['labels'])\n",
    "\n",
    "clusterBG_LTG = df1.loc[(df1['labels']==unique_labels[0])]\n",
    "BG_ETG = df1.loc[(df1['labels']==unique_labels[1])]\n",
    "clusterBG_edgeDisk = df1.loc[(df1['labels']==unique_labels[2])]\n",
    "likely_dE_ETGcluster = df1.loc[(df1['labels']==unique_labels[3])]\n",
    "likely_merging = df1.loc[(df1['labels']==unique_labels[4])]\n",
    "poss_dE_ETGcluster = df1.loc[(df1['labels']==unique_labels[5])]\n",
    "weak_bg = df1.loc[(df1['labels']==unique_labels[6])]\n",
    "\n",
    "downSampleDf0 = pd.concat([clusterBG_LTG, # 384\n",
    "                         BG_ETG.sample(frac = 400/3008),\n",
    "                         clusterBG_edgeDisk.sample(frac = 400/1049),\n",
    "                         likely_dE_ETGcluster, # 398\n",
    "                         likely_merging, # 23\n",
    "                         poss_dE_ETGcluster, # 98\n",
    "                         weak_bg # 477\n",
    "                         ])\n",
    "# checkBalance(downSampleDf0)\n",
    "\n",
    "def replace(df, ind):\n",
    "    label = list(df['labels'])[0]\n",
    "    newDf = df.replace(label, str(ind))\n",
    "    return(newDf)\n",
    "\n",
    "\n",
    "# combined 3 and 5\n",
    "second = pd.concat([\n",
    "                    replace(likely_dE_ETGcluster, 1), # old 3\n",
    "                    replace(poss_dE_ETGcluster, 1) # old 5\n",
    "                    ])\n",
    "# combine 0,1,2,6\n",
    "first = pd.concat([replace(clusterBG_LTG, 0), # old 0\n",
    "                   replace(BG_ETG, 0), # old 1\n",
    "                   replace(clusterBG_edgeDisk, 0), # old 2\n",
    "                   replace(weak_bg, 0) # old 6\n",
    "                    ])\n",
    "\n",
    "lenSecond = len(second.index)\n",
    "lenFirst = len(first.index)\n",
    "\n",
    "downSampleDf1 = pd.concat([first.sample(frac = lenSecond/lenFirst), second])\n",
    "\n",
    "# 0 is background\n",
    "# 1 is dE/ETGcluster\n",
    "\n",
    "\n",
    "# remove red contaminants\n",
    "downFiles = downSampleDf1['files']\n",
    "redPercent = [None]*len(downFiles)\n",
    "counter = 0\n",
    "workDir = 'SDSS-png/'\n",
    "\n",
    "# lower boundary RED color range values; Hue (0 - 10)\n",
    "lower1 = np.array([0, 80, 20])\n",
    "upper1 = np.array([10, 255, 255])\n",
    " \n",
    "# upper boundary RED color range values; Hue (160 - 180)\n",
    "lower2 = np.array([160, 100, 20])\n",
    "upper2 = np.array([179, 255, 255])\n",
    "\n",
    "for i, x in enumerate(downFiles):\n",
    "    testImgPath = os.path.join(workDir, x)\n",
    "    image = cv2.imread(testImgPath)\n",
    "    result = image.copy()\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # check image for pixels on the lower and upper end of hsv (red is weird for hsv)\n",
    "    lower_mask = cv2.inRange(image, lower1, upper1)\n",
    "    upper_mask = cv2.inRange(image, lower2, upper2)\n",
    "    full_mask = lower_mask + upper_mask;\n",
    "\n",
    "    result = cv2.bitwise_and(result, result, mask=full_mask)\n",
    "    dim = np.shape(full_mask)[0] \n",
    "    counts = np.count_nonzero(full_mask)\n",
    "    percent = 100*counts/dim**2\n",
    "    redPercent[i] = percent\n",
    "    subtitle_string = f'{percent}% of the image is red'\n",
    "    filename = testImgPath.split('\\\\')[-1]\n",
    "\n",
    "downSampleDf1['reds'] = redPercent # add new column of the red percentage of an image\n",
    "# downSampleDf1\n",
    "redList = (downSampleDf1['reds'] >= 50) # percentage threshold of how much red is in the image\n",
    "# downSampleDf1.shape\n",
    "df_filtered = downSampleDf1[downSampleDf1['reds'] <= 50]\n",
    "df_filtered.shape\n",
    "\n",
    "\n",
    "## Some visualizing snippets\n",
    "# redInds = np.where(redList)[0] # the indices of the hot pixel images to be removed\n",
    "# print(len(redInds))\n",
    "# for n in redInds:\n",
    "#     red = downSampleDf1['files'].to_list()[n]\n",
    "#     imStr = 'SDSS-png/' + red\n",
    "#     im = cv2.imread(imStr)[:,:,::-1] # [:,:,::-1] switches rgb to bgr and vice versa\n",
    "#     plt.figure\n",
    "#     plt.imshow(im)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c298f9ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Cluster or background LTG',\n",
       "       'Likely background ETG or unresolved source',\n",
       "       'Likely cluster or background edge-on disk galaxy',\n",
       "       'Likely dE/ETGcluster candidate', 'Likely merging system',\n",
       "       'Possible dE/ETGcluster candidate',\n",
       "       'background galaxy with possibly weak substructure'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67b30945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PCC-0006</td>\n",
       "      <td>49.2388</td>\n",
       "      <td>41.4631</td>\n",
       "      <td>19.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.01</td>\n",
       "      <td>19.28</td>\n",
       "      <td>1.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Likely merging system</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://dc.zah.uni-heidelberg.de/pcc/q/stamp/dl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PCC-0007</td>\n",
       "      <td>49.2392</td>\n",
       "      <td>41.4215</td>\n",
       "      <td>19.26</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.05</td>\n",
       "      <td>20.76</td>\n",
       "      <td>3.79</td>\n",
       "      <td>...</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Likely background ETG or unresolved source</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://dc.zah.uni-heidelberg.de/pcc/q/stamp/dl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PCC-0008</td>\n",
       "      <td>49.2411</td>\n",
       "      <td>41.4991</td>\n",
       "      <td>18.51</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.02</td>\n",
       "      <td>20.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>...</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cluster or background LTG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://dc.zah.uni-heidelberg.de/pcc/q/stamp/dl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PCC-0011</td>\n",
       "      <td>49.2420</td>\n",
       "      <td>41.4454</td>\n",
       "      <td>19.19</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.02</td>\n",
       "      <td>21.95</td>\n",
       "      <td>1.02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cluster or background LTG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://dc.zah.uni-heidelberg.de/pcc/q/stamp/dl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PCC-0026</td>\n",
       "      <td>49.2466</td>\n",
       "      <td>41.4451</td>\n",
       "      <td>19.34</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>21.31</td>\n",
       "      <td>1.20</td>\n",
       "      <td>...</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Likely cluster or background edge-on disk galaxy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://dc.zah.uni-heidelberg.de/pcc/q/stamp/dl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5386</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PCC-5387</td>\n",
       "      <td>49.9985</td>\n",
       "      <td>41.3856</td>\n",
       "      <td>18.60</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.84</td>\n",
       "      <td>0.06</td>\n",
       "      <td>22.78</td>\n",
       "      <td>2.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cluster or background LTG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://dc.zah.uni-heidelberg.de/pcc/q/stamp/dl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5407</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PCC-5408</td>\n",
       "      <td>50.0018</td>\n",
       "      <td>41.6806</td>\n",
       "      <td>18.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.85</td>\n",
       "      <td>3.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Likely background ETG or unresolved source</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://dc.zah.uni-heidelberg.de/pcc/q/stamp/dl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5416</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PCC-5417</td>\n",
       "      <td>50.0028</td>\n",
       "      <td>41.3384</td>\n",
       "      <td>18.84</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.27</td>\n",
       "      <td>0.02</td>\n",
       "      <td>20.74</td>\n",
       "      <td>2.42</td>\n",
       "      <td>...</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Likely background ETG or unresolved source</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://dc.zah.uni-heidelberg.de/pcc/q/stamp/dl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5422</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PCC-5423</td>\n",
       "      <td>50.0040</td>\n",
       "      <td>41.3410</td>\n",
       "      <td>17.50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.86</td>\n",
       "      <td>0.02</td>\n",
       "      <td>20.45</td>\n",
       "      <td>4.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Likely background ETG or unresolved source</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://dc.zah.uni-heidelberg.de/pcc/q/stamp/dl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5427</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PCC-5428</td>\n",
       "      <td>50.0051</td>\n",
       "      <td>41.6557</td>\n",
       "      <td>17.83</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.82</td>\n",
       "      <td>0.04</td>\n",
       "      <td>21.88</td>\n",
       "      <td>2.69</td>\n",
       "      <td>...</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cluster or background LTG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://dc.zah.uni-heidelberg.de/pcc/q/stamp/dl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>272 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0         1        2        3      4     5     6     7      8     9   \\\n",
       "5    NaN  PCC-0006  49.2388  41.4631  19.03  0.00  0.69  0.01  19.28  1.35   \n",
       "6    NaN  PCC-0007  49.2392  41.4215  19.26  0.03  0.97  0.05  20.76  3.79   \n",
       "7    NaN  PCC-0008  49.2411  41.4991  18.51  0.01  1.78  0.02  20.95  2.50   \n",
       "10   NaN  PCC-0011  49.2420  41.4454  19.19  0.01  1.51  0.02  21.95  1.02   \n",
       "25   NaN  PCC-0026  49.2466  41.4451  19.34  0.01  2.04  0.02  21.31  1.20   \n",
       "...   ..       ...      ...      ...    ...   ...   ...   ...    ...   ...   \n",
       "5386 NaN  PCC-5387  49.9985  41.3856  18.60  0.01  2.84  0.06  22.78  2.45   \n",
       "5407 NaN  PCC-5408  50.0018  41.6806  18.16  0.00  0.16  0.00  15.85  3.30   \n",
       "5416 NaN  PCC-5417  50.0028  41.3384  18.84  0.01  1.27  0.02  20.74  2.42   \n",
       "5422 NaN  PCC-5423  50.0040  41.3410  17.50  0.01  1.86  0.02  20.45  4.00   \n",
       "5427 NaN  PCC-5428  50.0051  41.6557  17.83  0.01  2.82  0.04  21.88  2.69   \n",
       "\n",
       "      ...    15    16    17    18   19   20  \\\n",
       "5     ...  0.39 -0.11   NaN   NaN  NaN  NaN   \n",
       "6     ...  1.02  0.68  0.88  0.66  NaN  NaN   \n",
       "7     ...  1.01  0.82  0.93  0.66  NaN  NaN   \n",
       "10    ...  0.83  0.59  0.70  0.47  NaN  NaN   \n",
       "25    ...  1.18  0.85  0.98  0.75  NaN  NaN   \n",
       "...   ...   ...   ...   ...   ...  ...  ...   \n",
       "5386  ...  0.88  0.70  0.82  0.65  NaN  NaN   \n",
       "5407  ...  0.24  0.12   NaN   NaN  NaN  NaN   \n",
       "5416  ...  1.05  0.75  0.78  0.71  NaN  NaN   \n",
       "5422  ...  0.92  0.76  0.93  0.69  NaN  NaN   \n",
       "5427  ...  0.84  0.66  0.73  0.56  NaN  NaN   \n",
       "\n",
       "                                                    21   22   23  \\\n",
       "5                                Likely merging system  NaN  NaN   \n",
       "6           Likely background ETG or unresolved source  NaN  NaN   \n",
       "7                            Cluster or background LTG  NaN  NaN   \n",
       "10                           Cluster or background LTG  NaN  NaN   \n",
       "25    Likely cluster or background edge-on disk galaxy  NaN  NaN   \n",
       "...                                                ...  ...  ...   \n",
       "5386                         Cluster or background LTG  NaN  NaN   \n",
       "5407        Likely background ETG or unresolved source  NaN  NaN   \n",
       "5416        Likely background ETG or unresolved source  NaN  NaN   \n",
       "5422        Likely background ETG or unresolved source  NaN  NaN   \n",
       "5427                         Cluster or background LTG  NaN  NaN   \n",
       "\n",
       "                                                     24  \n",
       "5     http://dc.zah.uni-heidelberg.de/pcc/q/stamp/dl...  \n",
       "6     http://dc.zah.uni-heidelberg.de/pcc/q/stamp/dl...  \n",
       "7     http://dc.zah.uni-heidelberg.de/pcc/q/stamp/dl...  \n",
       "10    http://dc.zah.uni-heidelberg.de/pcc/q/stamp/dl...  \n",
       "25    http://dc.zah.uni-heidelberg.de/pcc/q/stamp/dl...  \n",
       "...                                                 ...  \n",
       "5386  http://dc.zah.uni-heidelberg.de/pcc/q/stamp/dl...  \n",
       "5407  http://dc.zah.uni-heidelberg.de/pcc/q/stamp/dl...  \n",
       "5416  http://dc.zah.uni-heidelberg.de/pcc/q/stamp/dl...  \n",
       "5422  http://dc.zah.uni-heidelberg.de/pcc/q/stamp/dl...  \n",
       "5427  http://dc.zah.uni-heidelberg.de/pcc/q/stamp/dl...  \n",
       "\n",
       "[272 rows x 25 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brightDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a085f123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# want to clean /rotations-png/test and /rotations-png/train/ every run \n",
    "imgDirectory = 'rotations-png'\n",
    "testPath = os.path.join(imgDirectory,'test','*')\n",
    "testImgs = glob.glob(testPath)\n",
    "trainPath = os.path.join(imgDirectory,'train','*')\n",
    "trainImgs = glob.glob(trainPath)\n",
    "\n",
    "# testImgs = glob.glob(testDir)\n",
    "for x in testImgs:\n",
    "    os.remove(x)\n",
    "# can't do it all in one loop since in wrong dir\n",
    "for y in trainImgs:\n",
    "    os.remove(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f2e18f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate Rotation data\n",
    "\n",
    "def applyRotations(originalDf, outDir, greyFlag):\n",
    "\n",
    "    # files and labels as numpy arrays\n",
    "    files = originalDf['files'].to_numpy()\n",
    "    label = originalDf['labels'].to_numpy()\n",
    "    #rotDir = 'rotations'\n",
    "    rotDir = 'rotations-png'\n",
    "    originalDir = 'SDSS-png/'\n",
    "    # rotDir = 'rotations256'\n",
    "\n",
    "    rotFilenames = list()\n",
    "    rotLabels = list()\n",
    "\n",
    "    #angle = [90, 180, 270, 360]\n",
    "    angle = [30, 45, 60, 90, 120, 135, 150, 180, 210, 225, 240, 270, 300, 315, 330, 360]\n",
    "\n",
    "    # Use PIL to rotate image on angles in list\n",
    "    for ang in angle:\n",
    "        for f, l in zip(files, label):\n",
    "            imgString = originalDir + f\n",
    "            im = PIL.Image.open(imgString)\n",
    "            \n",
    "            if greyFlag == True:\n",
    "                im = im.convert('L')\n",
    "            out = im.rotate(ang)\n",
    "           \n",
    "            # generated filename\n",
    "            outString = f'{rotDir}/{outDir}/{f[:-5]}_rot{ang}_label={l}.png'\n",
    "            \n",
    "            # filename relative to working directory\n",
    "            dfString = f'{outDir}/{f[:-5]}_rot{ang}_label={l}.png'\n",
    "\n",
    "            out.save(outString)\n",
    "            rotFilenames.append(dfString)\n",
    "            rotLabels.append(l)\n",
    "\n",
    "            rotationDf = pd.DataFrame({'files': rotFilenames,\n",
    "                                    'labels': rotLabels})\n",
    "\n",
    "    return(rotationDf)\n",
    "\n",
    "# # Train/Test Split \n",
    "\n",
    "X = df_filtered['files']\n",
    "y = df_filtered['labels']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "trainDf = pd.DataFrame({'files' : X_train,\n",
    "                        'labels': y_train})\n",
    "testDf = pd.DataFrame({'files' : X_test,\n",
    "                        'labels': y_test})\n",
    "greyFlag = False\n",
    "trainDf_rot = applyRotations(trainDf, 'train', greyFlag)\n",
    "testDf_rot = applyRotations(testDf, 'test', greyFlag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4b958c4-d43e-489e-ad2c-bd7e0dae91b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1456\n",
      "1    1328\n",
      "Name: labels, dtype: int64\n",
      "26.15 %\n",
      "23.85 %\n"
     ]
    }
   ],
   "source": [
    "# checkBalance(trainDf)\n",
    "checkBalance(trainDf_rot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c5ff4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2064 validated image filenames belonging to 2 classes.\n",
      "Found 688 validated image filenames belonging to 2 classes.\n",
      "Found 928 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "# # Create datasets with flow from dataframe\n",
    "IMG_WIDTH = 200\n",
    "IMG_HEIGHT = 200\n",
    "TRAIN_BATCH_SIZE = 20\n",
    "VAL_BATCH_SIZE = 20\n",
    "\n",
    "#imgDirectory = \"./rotations/\"\n",
    "imgDirectory = \"./rotations-png/\"\n",
    "datagen=ImageDataGenerator(rescale=1./255.,validation_split=0.25)\n",
    "\n",
    "train_generator=datagen.flow_from_dataframe(\n",
    "dataframe=trainDf_rot,\n",
    "directory=imgDirectory,\n",
    "x_col=\"files\",\n",
    "y_col=\"labels\",\n",
    "subset=\"training\",\n",
    "batch_size=TRAIN_BATCH_SIZE, # divisibility\n",
    "seed=42,\n",
    "shuffle=True,\n",
    "class_mode=\"categorical\",\n",
    "target_size=(IMG_WIDTH,IMG_HEIGHT))\n",
    "\n",
    "validation_generator=datagen.flow_from_dataframe(\n",
    "dataframe=trainDf_rot,\n",
    "directory=imgDirectory,\n",
    "x_col=\"files\",\n",
    "y_col=\"labels\",\n",
    "subset=\"validation\",\n",
    "batch_size=VAL_BATCH_SIZE,\n",
    "seed=42,\n",
    "shuffle=True,\n",
    "class_mode=\"categorical\",\n",
    "target_size=(IMG_WIDTH,IMG_HEIGHT))\n",
    "\n",
    "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
    "test_generator=test_datagen.flow_from_dataframe(\n",
    "dataframe=testDf_rot,\n",
    "directory=imgDirectory,\n",
    "x_col=\"files\",\n",
    "y_col=None,\n",
    "batch_size=1,\n",
    "seed=42,\n",
    "shuffle=False,\n",
    "class_mode=None,\n",
    "target_size=(IMG_WIDTH,IMG_HEIGHT))\n",
    "\n",
    "\n",
    "# # ResNet50 Model\n",
    "#\n",
    "# https://github.com/suvoooo/Learn-TensorFlow/blob/master/resnet/Implement_Resnet_TensorFlow.ipynb\n",
    "\n",
    "def res_identity(x, filters):\n",
    "    x_skip = x # this will be used for addition with the residual block\n",
    "    f1, f2 = filters\n",
    "\n",
    "    #first block\n",
    "    x = Conv2D(f1, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activations.relu)(x)\n",
    "\n",
    "    #second block # bottleneck (but size kept same with padding)\n",
    "    x = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activations.relu)(x)\n",
    "\n",
    "    # third block activation used after adding the input\n",
    "    x = Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    # x = Activation(activations.relu)(x)\n",
    "\n",
    "    # add the input\n",
    "    x = Add()([x, x_skip])\n",
    "    x = Activation(activations.relu)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def res_conv(x, s, filters):\n",
    "\n",
    "    x_skip = x\n",
    "    f1, f2 = filters\n",
    "\n",
    "    # first block\n",
    "    x = Conv2D(f1, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=l2(0.001))(x)\n",
    "    # when s = 2 then it is like downsizing the feature map\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activations.relu)(x)\n",
    "\n",
    "    # second block\n",
    "    x = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activations.relu)(x)\n",
    "\n",
    "    #third block\n",
    "    x = Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # shortcut\n",
    "    x_skip = Conv2D(f2, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=l2(0.001))(x_skip)\n",
    "    x_skip = BatchNormalization()(x_skip)\n",
    "\n",
    "    # add\n",
    "    x = Add()([x, x_skip])\n",
    "    x = Activation(activations.relu)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet50():\n",
    "\n",
    "    input_im = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3)) # cifar 10 images size\n",
    "    x = ZeroPadding2D(padding=(3, 3))(input_im)\n",
    "\n",
    "    # 1st stage\n",
    "    # here we perform maxpooling, see the figure above\n",
    "\n",
    "    x = Conv2D(64, kernel_size=(7, 7), strides=(2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activations.relu)(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    #2nd stage\n",
    "    # frm here on only conv block and identity block, no pooling\n",
    "\n",
    "    x = res_conv(x, s=1, filters=(64, 256))\n",
    "    x = res_identity(x, filters=(64, 256))\n",
    "    x = res_identity(x, filters=(64, 256))\n",
    "\n",
    "    # 3rd stage\n",
    "\n",
    "    x = res_conv(x, s=2, filters=(128, 512))\n",
    "    x = res_identity(x, filters=(128, 512))\n",
    "    x = res_identity(x, filters=(128, 512))\n",
    "    x = res_identity(x, filters=(128, 512))\n",
    "\n",
    "    # 4th stage\n",
    "\n",
    "    x = res_conv(x, s=2, filters=(256, 1024))\n",
    "    x = res_identity(x, filters=(256, 1024))\n",
    "    x = res_identity(x, filters=(256, 1024))\n",
    "    x = res_identity(x, filters=(256, 1024))\n",
    "    x = res_identity(x, filters=(256, 1024))\n",
    "    x = res_identity(x, filters=(256, 1024))\n",
    "\n",
    "    # 5th stage\n",
    "\n",
    "    x = res_conv(x, s=2, filters=(512, 2048))\n",
    "    x = res_identity(x, filters=(512, 2048))\n",
    "    x = res_identity(x, filters=(512, 2048))\n",
    "\n",
    "    # ends with average pooling and dense connection\n",
    "\n",
    "    x = AveragePooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(2, activation='softmax', kernel_initializer='he_normal')(x) #multi-class\n",
    "\n",
    "    # define the model\n",
    "\n",
    "    model = Model(inputs=input_im, outputs=x, name='Resnet50')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38e1963",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = resnet50()\n",
    "# BATCH_SIZE = 1\n",
    "\n",
    "### Hyperparameters ###\n",
    "n_epochs = 300\n",
    "init_lr = 7.5e-2\n",
    "# init_lr = float(sys.argv[1])\n",
    "decay_rate = 0.99\n",
    "# decay_rate = float(sys.argv[2])\n",
    "decay_steps = 100_000\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                    initial_learning_rate = init_lr,\n",
    "                    decay_steps = decay_steps,\n",
    "                    decay_rate = decay_rate)\n",
    "\n",
    "\n",
    "cnn_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule),\n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "history = cnn_model.fit(train_generator,\n",
    "                        epochs = n_epochs,\n",
    "                        callbacks = [es],\n",
    "                        verbose = 0,\n",
    "                        validation_data=validation_generator)\n",
    "\n",
    "\n",
    "\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# learning_rate = history.history['']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfce8760",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(12, 10))\n",
    "\n",
    "ax[0].set_title('Training Accuracy vs. Epochs')\n",
    "ax[0].plot(train_accuracy, 'o-', label='Train Accuracy')\n",
    "ax[0].plot(val_accuracy, 'o-', label='Validation Accuracy')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].legend(loc='best')\n",
    "ax[0].grid()\n",
    "\n",
    "ax[1].set_title('Training/Validation Loss vs. Epochs')\n",
    "ax[1].plot(train_loss, 'o-', label='Train Loss')\n",
    "ax[1].plot(val_loss, 'o-', label='Validation Loss')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].set_ylim([-1, 100])\n",
    "ax[1].legend(loc='best')\n",
    "\n",
    "# ax[2].set_title('Learning Rate vs. Epochs')\n",
    "# ax[2].plot(learning_rate, 'o-', label='Learning Rate')\n",
    "# ax[2].set_xlabel('Epochs')\n",
    "# ax[2].set_ylabel('Learning Rate')\n",
    "# ax[2].legend(loc='best')\n",
    "\n",
    "# ax[3].set_title('Loss vs learning rate')\n",
    "# # ax[3].plot(learning_rate, 'o-', label='Learning Rate')\n",
    "# ax[3].plot(learning_rate, train_loss, 'o-', label='Train Loss')\n",
    "# ax[3].plot(learning_rate, val_loss, 'o-', label='Validation Loss')\n",
    "# ax[3].set_ylabel('Loss')\n",
    "# ax[3].set_xlabel('Learning Rate')\n",
    "# ax[3].legend(loc='best')\n",
    "\n",
    "plt.suptitle(f'Initial LR = {init_lr} Decay Rate = {decay_rate}')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'train-report_init-lr={init_lr}_decay-rate={decay_rate}_offrots.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(nrows=2, ncols=5, figsize=(12, 10))\n",
    "# idx = 0\n",
    "\n",
    "# for i in range(2):\n",
    "#     for j in range(5):\n",
    "#         predicted_label = unique_labels[np.argmax(predictions[idx])]\n",
    "#         ax[i, j].set_title(f\"{predicted_label}\", fontsize=10)\n",
    "#         ax[i, j].imshow(test_generator[idx][0])\n",
    "\n",
    "#         ax[i, j].axis(\"off\")\n",
    "#         idx += 1\n",
    "\n",
    "# # plt.tight_layout()\n",
    "# plt.suptitle(\"Test Dataset Predictions\", fontsize=20)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# In[34]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3c5d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cnn_model.predict(test_generator)\n",
    "test_loss, test_accuracy = cnn_model.evaluate(validation_generator, batch_size=1) # needs to be divisible\n",
    "\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_true = testDf_rot['labels'] # this needs to be checked if you change the input dataframes\n",
    "y_true = y_true.tolist()\n",
    "# len(y_pred) == len(y_true)\n",
    "unique_labels = {value: key for key, value in train_generator.class_indices.items()}\n",
    "\n",
    "# print(\"Label Mappings for classes present in the training and validation datasets\\n\")\n",
    "# for key, value in unique_labels.items():\n",
    "#     print(f\"{key} : {value}\")\n",
    "\n",
    "# function to return key for any value\n",
    "def get_key(val):\n",
    "    for key, value in unique_labels.items():\n",
    "        if val == value:\n",
    "            return key\n",
    "\n",
    "    return \"key doesn't exist\"\n",
    "\n",
    "Y_true = []\n",
    "# for i in range(len(y_true)): # This was the original way to do it -- be careful, this only solved a mismatch and could be wrong\n",
    "for i in range(len(y_pred)):\n",
    "    Y_true.append(get_key(y_true[i]))\n",
    "\n",
    "cf_mtx = confusion_matrix(Y_true, y_pred)\n",
    "\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in cf_mtx.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in cf_mtx.flatten()/np.sum(cf_mtx)]\n",
    "box_labels = [f\"{v1}\\n({v2})\" for v1, v2 in zip(group_counts, group_percentages)]\n",
    "box_labels = np.asarray(box_labels).reshape(2, 2)\n",
    "\n",
    "\n",
    "# cf_mtx.sum()\n",
    "\n",
    "y_true = np.array([int(x) for x in y_true]) # cast to np array for type consistency with y_pred\n",
    "errors = (y_true - y_pred != 0) # everywhere the numbers don't match\n",
    "y_true_errors = y_true[errors]\n",
    "y_pred_errors = y_pred[errors]\n",
    "\n",
    "test_images = test_generator.filenames\n",
    "test_img_err = np.asarray(test_images)[errors]\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(nrows=3, ncols=5, figsize=(12, 10))\n",
    "# idx = 0\n",
    "\n",
    "hits = (y_true - y_pred == 0)\n",
    "y_true_hits = y_true[hits]\n",
    "y_pred_hits = y_pred[hits]\n",
    "\n",
    "test_img_hits = np.asarray(test_images)[hits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce50a88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 10))\n",
    "# sns.heatmap(cf_mtx, xticklabels=labels.values(), yticklabels=labels.values(),\n",
    "#            cmap=\"YlGnBu\", fmt=\"\", annot=box_labels)\n",
    "sns.heatmap(cf_mtx, cmap=\"YlGnBu\", fmt=\"\", annot=box_labels)\n",
    "plt.xlabel('Predicted Classes')\n",
    "plt.ylabel('True Classes')\n",
    "#plt.show()\n",
    "\n",
    "plt.title(f\"Test Accuracy: {test_accuracy*100:.2f}\")\n",
    "plt.savefig(f'confusion-matrix_init-lr={init_lr}_decay-rate={decay_rate}_offrots.png')\n",
    "\n",
    "# for i in range(3):\n",
    "#     for j in range(5):\n",
    "#         idx = np.random.randint(0, len(test_img_err))\n",
    "#         true_index = y_true_errors[idx]\n",
    "#         true_label = unique_labels[true_index]\n",
    "#         predicted_index = y_pred_errors[idx]\n",
    "#         predicted_label = unique_labels[predicted_index]\n",
    "#         ax[i, j].set_title(f\"True Label: {true_label} \\n Predicted Label: {predicted_label}\")\n",
    "#         ax[i, j].imshow(test_generator[idx][0])\n",
    "#         ax[i, j].axis(\"off\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.suptitle('Wrong Predictions made on test set', fontsize=15)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3, ncols=5, figsize=(12, 10))\n",
    "idx = 0\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(5):\n",
    "        idx = np.random.randint(0, len(test_img_hits))\n",
    "        true_index = y_true_hits[idx]\n",
    "        true_label = unique_labels[true_index]\n",
    "        predicted_index = y_pred_hits[idx]\n",
    "        predicted_label = unique_labels[predicted_index]\n",
    "        ax[i, j].set_title(f\"True Label: {true_label} \\n Predicted Label: {predicted_label}\")\n",
    "        ax[i, j].imshow(test_generator[idx][0])\n",
    "        ax[i, j].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('True Predictions made on test set', fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "# # End result:\n",
    "print(f'Init conds:')\n",
    "print(f'init lr: {init_lr}, decay rate: {decay_rate}, decay steps: {decay_steps}')\n",
    "print(f\"Test Loss:     {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9725be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = [None]*53\n",
    "ind = 0\n",
    "for layer in cnn_model.layers:\n",
    "    \n",
    "    # check for convolutional layer\n",
    "    if 'conv' not in layer.name:\n",
    "        continue\n",
    "        \n",
    "    else:\n",
    "        # get filter weights\n",
    "        filters, biases = layer.get_weights()\n",
    "        print(layer.name, filters.shape, layer.output.shape)\n",
    "        conv_layers[ind] = layer.name\n",
    "        ind += 1\n",
    "len(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626ffbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters, biases = cnn_model.layers[9].get_weights()\n",
    "\n",
    "# normalize filter values to 0-1 so we can visualize them\n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "filters = (filters - f_min) / (f_max - f_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aec2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot first few filters\n",
    "n_filters, ix = 6, 1\n",
    "for i in range(n_filters):\n",
    "    # get the filter\n",
    "    f = filters[:, :, :, i]\n",
    "    \n",
    "# plot each channel separately\n",
    "for j in range(3):\n",
    "    # specify subplot and turn of axis\n",
    "    ax = plt.subplot(n_filters, 3, ix)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    # plot filter channel in grayscale\n",
    "    plt.imshow(f[:, :, j], cmap='gray')\n",
    "    ix += 1\n",
    "    # show the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f52850",
   "metadata": {},
   "outputs": [],
   "source": [
    "## are these redundant?? TODO -- Clean!\n",
    "\n",
    "trainPath = os.path.join('rotations-png', 'train', '*')\n",
    "trainImgs = glob.glob(trainPath)\n",
    "testPath = os.path.join('rotations-png', 'test', '*')\n",
    "testImgs = glob.glob(testPath)\n",
    "\n",
    "maps_model = Model(cnn_model.inputs, outputs=cnn_model.layers[9].output)\n",
    "maps_model.summary()\n",
    "exImage = load_img(testImgs[0])\n",
    "copyImg = exImage.copy()\n",
    "exImage = img_to_array(exImage)\n",
    "exImage = np.expand_dims(exImage, axis = 0)\n",
    "exImage = preprocess_input(exImage)\n",
    "\n",
    "feature_maps = maps_model.predict(exImage)\n",
    "# feature_maps\n",
    "# np.shape(exImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fca431",
   "metadata": {},
   "outputs": [],
   "source": [
    "square = 8\n",
    "ix = 1\n",
    "for _ in range(square):\n",
    "    for _ in range(square):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = plt.subplot(square, square, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        plt.imshow(feature_maps[0, :, :, ix-1], cmap='gray')\n",
    "#         plt.imshow((feature_maps[0, :, :, ix-1]))\n",
    "        ix += 1\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28b4c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(copyImg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f61c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testImgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b6bb19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
